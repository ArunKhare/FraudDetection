{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\arunk'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====>>>File already exists\n"
     ]
    }
   ],
   "source": [
    "from fraudDetection.logger import logging\n",
    "from fraudDetection.exception import FraudDetectionException\n",
    "from fraudDetection.config.configuration import ConfigurationManager\n",
    "from fraudDetection.components import DataIngestion, DataValidation, DataTransformation,ModelTrainer\n",
    "from fraudDetection.entity import DataIngestionArtifact, DataValidationArtifact, DataTransformationArtifact, ModelTrainerArtifact\n",
    "\n",
    "from fraudDetection.constants import CONFIG_FILE_PATH\n",
    "config = ConfigurationManager(CONFIG_FILE_PATH)\n",
    "data_ingestion_config = config.get_data_ingestion_config()\n",
    "data_ingestion_artifacts = DataIngestion(data_ingestion_config).initiate_data_ingestion()\n",
    "\n",
    "data_validation_config =config.get_data_validation_config()\n",
    "data_validation_artifacts =DataValidation(data_ingestion_artifacts,data_validation_config).initiate_data_validation()\n",
    "data_transformation_config =config.get_data_transformation_config()\n",
    "data_transformation_obj= DataTransformation(data_validation_artifacts,data_ingestion_artifacts,data_transformation_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 {color: black;background-color: white;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 pre{padding: 0;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-toggleable {background-color: white;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-item {z-index: 1;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-parallel-item:only-child::after {width: 0;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-d89fa3d3-0701-4a34-ae71-bd1e1d23c1b4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;transformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;ohe_type&#x27;,\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  BoxList([&#x27;type&#x27;])),\n",
       "                                                 (&#x27;num_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   RobustScaler())]),\n",
       "                                                  BoxList([&#x27;step&#x27;, &#x27;amount&#x27;, &#x27;oldbalanceOrg&#x27;, &#x27;newbalanceOrig&#x27;, &#x27;oldbalanceDest&#x27;, &#x27;newbalanceDest&#x27;]))])),\n",
       "                (&#x27;feature_extactor&#x27;, ExtraTreesRegressor())])</pre><b>Please rerun this cell to show the HTML repr or trust the notebook.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"07eebbb3-4872-4fe2-b812-adaa3661cb47\" type=\"checkbox\" ><label for=\"07eebbb3-4872-4fe2-b812-adaa3661cb47\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;transformer&#x27;,\n",
       "                 ColumnTransformer(transformers=[(&#x27;cat_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;ohe_type&#x27;,\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  BoxList([&#x27;type&#x27;])),\n",
       "                                                 (&#x27;num_pipeline&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scaler&#x27;,\n",
       "                                                                   RobustScaler())]),\n",
       "                                                  BoxList([&#x27;step&#x27;, &#x27;amount&#x27;, &#x27;oldbalanceOrg&#x27;, &#x27;newbalanceOrig&#x27;, &#x27;oldbalanceDest&#x27;, &#x27;newbalanceDest&#x27;]))])),\n",
       "                (&#x27;feature_extactor&#x27;, ExtraTreesRegressor())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cd336f27-a4fd-4c1d-8c47-0e640a4668ab\" type=\"checkbox\" ><label for=\"cd336f27-a4fd-4c1d-8c47-0e640a4668ab\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;cat_pipeline&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;ohe_type&#x27;,\n",
       "                                                  OneHotEncoder())]),\n",
       "                                 BoxList([&#x27;type&#x27;])),\n",
       "                                (&#x27;num_pipeline&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scaler&#x27;, RobustScaler())]),\n",
       "                                 BoxList([&#x27;step&#x27;, &#x27;amount&#x27;, &#x27;oldbalanceOrg&#x27;, &#x27;newbalanceOrig&#x27;, &#x27;oldbalanceDest&#x27;, &#x27;newbalanceDest&#x27;]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8f4fbcf1-ef80-41dc-936d-7bfa693b16d8\" type=\"checkbox\" ><label for=\"8f4fbcf1-ef80-41dc-936d-7bfa693b16d8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat_pipeline</label><div class=\"sk-toggleable__content\"><pre>[&#x27;type&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8fa79828-62f2-4911-9034-a3e9e1ba1df9\" type=\"checkbox\" ><label for=\"8fa79828-62f2-4911-9034-a3e9e1ba1df9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"75baf6cc-663b-4b17-b644-722052a6a323\" type=\"checkbox\" ><label for=\"75baf6cc-663b-4b17-b644-722052a6a323\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num_pipeline</label><div class=\"sk-toggleable__content\"><pre>[&#x27;step&#x27;, &#x27;amount&#x27;, &#x27;oldbalanceOrg&#x27;, &#x27;newbalanceOrig&#x27;, &#x27;oldbalanceDest&#x27;, &#x27;newbalanceDest&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"321a6630-bde1-4e19-992e-bc3f884031b7\" type=\"checkbox\" ><label for=\"321a6630-bde1-4e19-992e-bc3f884031b7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RobustScaler</label><div class=\"sk-toggleable__content\"><pre>RobustScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c077b8a8-d073-4c5d-9bbd-e79646ed3e02\" type=\"checkbox\" ><label for=\"c077b8a8-d073-4c5d-9bbd-e79646ed3e02\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('transformer',\n",
       "                 ColumnTransformer(transformers=[('cat_pipeline',\n",
       "                                                  Pipeline(steps=[('ohe_type',\n",
       "                                                                   OneHotEncoder())]),\n",
       "                                                  BoxList(['type'])),\n",
       "                                                 ('num_pipeline',\n",
       "                                                  Pipeline(steps=[('scaler',\n",
       "                                                                   RobustScaler())]),\n",
       "                                                  BoxList(['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']))])),\n",
       "                ('feature_extactor', ExtraTreesRegressor())])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformation_obj.get_data_transfomer_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arunk\\FraudDetection\\src\\fraudDetection\\components\\data_transformation.py:255: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df.fillna(df.median(),inplace=True)\n",
      "C:\\Users\\arunk\\FraudDetection\\src\\fraudDetection\\components\\data_transformation.py:255: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  df.fillna(df.median(),inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataTransformationArtifact(is_transformed=True, message='Transformed', transformed_train_file_path='c:\\\\Users\\\\arunk\\\\FraudDetection\\\\artifacts\\\\transformed_dir\\\\train\\\\file_0.npz', transformed_test_file_path='c:\\\\Users\\\\arunk\\\\FraudDetection\\\\artifacts\\\\transformed_dir\\\\test\\\\file_0.npz', processed_object_file_path=WindowsPath('c:/Users/arunk/FraudDetection/artifacts/transformed_dir/preprocessed/preprocessed.pkl'))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformation_obj.initiate_data_transformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ing_arti = data_transformation_obj.data_ingestion_artifact\n",
    "val_arti = data_transformation_obj.data_validation_artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'step': 'int64', 'type': 'object', 'amount': 'float64', 'oldbalanceOrg': 'float64', 'newbalanceOrig': 'float64', 'oldbalanceDest': 'float64', 'newbalanceDest': 'float64', 'isFraud': 'int64'}\n"
     ]
    }
   ],
   "source": [
    "from fraudDetection.utils import save_object, read_yaml, load_data, save_numpy_array_data\n",
    "from fraudDetection.constants import *\n",
    "schema_path = val_arti.schema_file_path\n",
    "schema = read_yaml(schema_path)\n",
    "cat_col = schema[DATA_SCHEMA_CATEGORICAL_COLUMN_KEY]\n",
    "col = schema[DATA_SCHEMA_COLUMNS_KEY]\n",
    "num_col = schema[DATA_SCHEMA_NUMERICAL_COLUMN_KEY]\n",
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data(ing_arti.train_file_path,col)\n",
    "test_df = load_data(ing_arti.test_file_path,col)\n",
    "target_col_name = schema[DATA_SCHEMA_TARGET_COLUMN_KEY][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39832, 7) (39832,)\n"
     ]
    }
   ],
   "source": [
    "input_train_df = train_df.drop(target_col_name,axis=1)\n",
    "target_col = train_df[target_col_name]\n",
    "print(input_train_df.shape, target_col.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.feature_selection import SelectPercentile, chi2, SelectFromModel\n",
    "from sklearn.utils.validation import check_is_fitted, check_X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the preprocessing steps for categorical and numerical columns\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "cat_preprocessor = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    # (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "])\n",
    "\n",
    "num_preprocessor = Pipeline(steps=[\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# define the preprocessor pipeline with SMOTENC and RandomOverSampler\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_preprocessor, cat_col),\n",
    "    ('num', num_preprocessor, num_col)\n",
    "],remainder=\"passthrough\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.impute import SimpleImputer\n",
    "# SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "\n",
    "class SMOTENCWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, categorical_features:list, sampling_strategy:float,target_feature_name:str):\n",
    "        self.categorical_features = categorical_features\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.target_feature_name = target_feature_name\n",
    "        self.smotenc = SMOTENC(sampling_strategy=sampling_strategy, categorical_features=categorical_features)\n",
    "        self.target_feature_name = target_feature_name\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_resampled_, self.y_resampled_ = self.smotenc.fit_resample(X, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None) -> pd.DataFrame:\n",
    "        df_resampled: pd.DataFrame = pd.concat([pd.DataFrame(self.X_resampled_), pd.Series(self.y_resampled_, name=self.target_feature_name)], axis=1)\n",
    "        return df_resampled\n",
    "    \n",
    "class RandomUnderSamplerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, sampling_strategy,target_feature_name):\n",
    "        self.sampling_strategy = sampling_strategy\n",
    "        self.sampler = RandomUnderSampler(sampling_strategy=sampling_strategy)\n",
    "        self.target_feature_name = target_feature_name\n",
    "    def fit(self, X, y):\n",
    "        self.X_resampled_, self.y_resampled_ = self.sampler.fit_resample(X.drop(columns=[self.target_feature_name]), X[self.target_feature_name])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None) -> pd.DataFrame:\n",
    "        df_resampled: pd.DataFrame = pd.concat([pd.DataFrame(self.X_resampled_), pd.Series(self.y_resampled_, name=self.target_feature_name)], axis=1)\n",
    "        return df_resampled\n",
    "        # return self.X_resampled_, self.y_resampled_\n",
    "    \n",
    "class FeatureExtractor(BaseEstimator,TransformerMixin):\n",
    "    def fit(self,X,y):\n",
    "        self.model = ExtraTreesRegressor()\n",
    "        self.model.fit(X,y)   \n",
    "        return self\n",
    "    def transform(self,X,y):\n",
    "        return self\n",
    "\n",
    "def get_data_balancing_object(target_feature_name) -> Pipeline:\n",
    "    \"\"\"\n",
    "    define the preprocessor pipeline with SMOTENC and RandomUnderSampler\n",
    "    Args: target feature column name\n",
    "    return: pipeline object     \n",
    "    \"\"\"\n",
    "    sampling_pipeline = Pipeline(steps=[\n",
    "    ('sampling', SMOTENCWrapper(categorical_features=[1], sampling_strategy=0.1,target_feature_name=target_feature_name)),\n",
    "    ('oversample', RandomUnderSamplerWrapper(sampling_strategy=0.5,target_feature_name=target_feature_name))\n",
    "    ])\n",
    "    return sampling_pipeline\n",
    "\n",
    "sampling_pipeline = Pipeline(steps=[\n",
    "    ('sampling', SMOTENCWrapper(categorical_features=[1], sampling_strategy=0.1,target_feature_name=target_col_name)),\n",
    "    ('oversample', RandomUnderSamplerWrapper(sampling_strategy=0.5,target_feature_name=target_col_name))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_strategy='median', cat_strategy='most_frequent', thresh=None):\n",
    "        self.num_strategy = num_strategy\n",
    "        self.cat_strategy = cat_strategy\n",
    "        self.thresh = thresh\n",
    "        self.num_imputer = SimpleImputer(strategy=self.num_strategy)\n",
    "        self.cat_imputer = SimpleImputer(strategy=self.cat_strategy)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.thresh is not None:\n",
    "            drop_cols = X.columns[X.isnull().mean() > self.thresh]\n",
    "            X.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "        num_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "        self.num_imputer.fit(X[num_cols])\n",
    "\n",
    "        cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        self.cat_imputer.fit(X[cat_cols])\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.thresh is not None:\n",
    "            drop_cols = X.columns[X.isnull().mean() > self.thresh]\n",
    "            X.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "        num_cols = X.select_dtypes(include=['float', 'int']).columns\n",
    "        X[num_cols] = self.num_imputer.transform(X[num_cols])\n",
    "\n",
    "        cat_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "        X[cat_cols] = self.cat_imputer.transform(X[cat_cols])\n",
    "\n",
    "        return X, self.y\n",
    "\n",
    "imputer = Pipeline(steps=[(\"CustomImputer\", CustomImputer(num_strategy='median', cat_strategy='most_frequent', thresh=.3))])\n",
    "# make_pipeline([(CustomImputer(num_strategy='median', cat_strategy='most_frequent', thresh=.3), set_output(transform=\"pandas\"))])\n",
    "column_name = train_df.columns[:-1]\n",
    "\n",
    "pipe_obj = Pipeline([\n",
    "    (\"CustomImputer\",imputer),\n",
    "    # (\"sampling_pipeline\", sampling_pipeline),\n",
    "    # (\"transformer\", preprocessor)\n",
    "    ])\n",
    "\n",
    "# print(train_df.columns)\n",
    "pipe_obj.fit_transform(input_train_df,target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "c:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_thresh, upper_thresh):\n",
    "        super().__init__()\n",
    "        self.lower_thresh = lower_thresh\n",
    "        self.upper_thresh = upper_thresh\n",
    " \n",
    "    def fit(self, X, y):\n",
    "        null_prop_ = X.isnull().sum()/len(X)\n",
    "        drop_cols= null_prop_[null_prop_ > self.upper_thresh].index \n",
    "        X.drop(columns=drop_cols,inplace= True)\n",
    "\n",
    "        drop_rows = X.loc[X.isnull().mean(axis=1).between(self.lower_thresh,self.upper_thresh)].index\n",
    "        X.drop(index=drop_rows, inplace=True)\n",
    "         \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # fill Age\n",
    "        X.fillna(X.median(),inplace=True)\n",
    "        return X\n",
    "\n",
    "imputer = Pipeline([(\"CustomImputer\",CustomImputer(lower_thresh=0.1,upper_thresh=0.3))])\n",
    "\n",
    "# class PipelineWithY(Pipeline):\n",
    "#     def transform(self, X, y=None):\n",
    "#         Xt = X\n",
    "#         for name, transform in self.steps[:-1]:\n",
    "#             # all transformations except the last one\n",
    "#             if hasattr(transform, 'transform'):\n",
    "#                 Xt = transform.transform(Xt)\n",
    "#         # last transformation, handle differently to get X and y\n",
    "#         name, last_transform = self.steps[-1]\n",
    "#         if hasattr(last_transform, 'transform'):\n",
    "#             Xt = last_transform.transform(Xt)\n",
    "#         # return as tuple of (X, y)\n",
    "#         return Xt, y\n",
    "\n",
    "pipe_obj = Pipeline([\n",
    "    (\"CustomImputer\",imputer),\n",
    "    (\"sampling_pipeline\", sampling_pipeline),\n",
    "    # ('target_extractor', FunctionTransformer(lambda df: (df.drop(columns=target_col_name), df[target_col_name]))),\n",
    "    # (\"transformer\", preprocessor)\n",
    "])\n",
    "\n",
    "pipe_obj.fit(input_train_df,target_col)\n",
    "df_resampled =pipe_obj.transform(input_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "c:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "FeatureExtractor = Pipeline(steps=[\n",
    "  ('feature_selection', SelectFromModel(ExtraTreesClassifier(n_estimators=50,max_features=\"log2\"),threshold=0.05))\n",
    "])\n",
    "\n",
    "pipe_obj.fit(input_train_df,target_col)\n",
    "df_resampled =pipe_obj.transform(input_train_df)\n",
    "\n",
    "XTrain = df_resampled.iloc[:,:-1]\n",
    "yTrain = df_resampled.iloc[:,-1]\n",
    "\n",
    "cat_preprocessor = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "    (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "])\n",
    "\n",
    "num_preprocessor = Pipeline(steps=[\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "# define the preprocessor pipeline with SMOTENC and RandomOverSampler\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_preprocessor, cat_col),\n",
    "    ('num', num_preprocessor, num_col)\n",
    "],remainder=\"passthrough\")\n",
    "\n",
    "pipe_obj1 = Pipeline(steps=[\n",
    "    (\"transformer\", preprocessor),\n",
    "    (\"FeatureExtractor\",FeatureExtractor)\n",
    "])\n",
    "arr =pipe_obj1.fit_transform(XTrain,yTrain)\n",
    "feature_out_name = pipe_obj1.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XTrain.shape: (11940, 7), YTrain.shape: (11940,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# np.c_[arr,np.array(yTrain)]\n",
    "\n",
    "XTrain = df_resampled.iloc[:,:-1]\n",
    "yTrain = df_resampled.iloc[:,-1]\n",
    "print(f\"XTrain.shape: {XTrain.shape}, YTrain.shape: {yTrain.shape}\" )\n",
    "yTrain.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\ipykernel_launcher.py:21: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11940, 7) (11940,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (11940,7) into shape (11940,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13436\\2528176979.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# fit and transform the data using the combined pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_train_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mfeature_out_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FeatureExtractor\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    424\u001b[0m         \"\"\"\n\u001b[0;32m    425\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    353\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m             )\n\u001b[0;32m    357\u001b[0m             \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;31m# set n_features_in_ attribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__array__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\arunk\\FraudDetection\\env\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (11940,7) into shape (11940,)"
     ]
    }
   ],
   "source": [
    "# define a function to convert the output of pipe_obj into XTrain and yTrain\n",
    "def split_data(df):\n",
    "    X_train = df.iloc[:,:-1]\n",
    "    y_train = df.iloc[:,-1]\n",
    "    print(X_train.shape,y_train.shape)\n",
    "    return y_train, X_train\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_preprocessor, cat_col),\n",
    "    ('num', num_preprocessor, num_col)\n",
    "],remainder=\"passthrough\")\n",
    "\n",
    "pipe_obj1 = Pipeline(steps=[\n",
    "    (\"transformer\", preprocessor),\n",
    "    (\"FeatureExtractor\",FeatureExtractor)\n",
    "])\n",
    "\n",
    "# define the combined pipeline\n",
    "combined_pipeline = Pipeline([\n",
    "    (\"CustomImputer\",imputer),\n",
    "    (\"sampling_pipeline\", sampling_pipeline),\n",
    "    (\"split_data\", FunctionTransformer(split_data)),\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"FeatureExtractor\", FeatureExtractor)\n",
    "])\n",
    "\n",
    "# fit and transform the data using the combined pipeline\n",
    "arr = combined_pipeline.fit_transform(input_train_df, target_col,)\n",
    "feature_out_name = combined_pipeline.named_steps[\"FeatureExtractor\"].get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.232882   9.74079297]\n",
      " [4.69067116 5.22841156]\n",
      " [3.41057392 9.02033036]]\n",
      "[5.232882   4.69067116 3.41057392]\n",
      "(3,)\n",
      "[[5.232882  ]\n",
      " [4.69067116]\n",
      " [3.41057392]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.random_sample((3,2))*10\n",
    "print(X)\n",
    "print(X[:,0])\n",
    "print(X[:,0].shape)\n",
    "print(X[:,0].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.groupby(df.dtypes)\n",
    "df.columns.get_loc(\"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_col\n",
    "num_col\n",
    "target_col_name\n",
    "column_name\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "cat_selector = make_column_selector(dtype_include=object)\n",
    "num_selector =make_column_selector(dtype_include=np.number)\n",
    "\n",
    "cat_tree_processor = OneHotEncoder()\n",
    "num_tree_processor = SimpleImputer(strategy='median')\n",
    "\n",
    "tree_processor = make_column_transformer(\n",
    "    (cat_preprocessor,cat_selector),\n",
    "    (num_preprocessor,num_selector))\n",
    "\n",
    "cat_linear_processor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "num_linear_processor = make_pipeline(\n",
    "    RobustScaler(),SimpleImputer(strategy='median')\n",
    ")\n",
    "linear_preprocessor = make_column_transformer(\n",
    "    (num_linear_processor,num_selector), (cat_linear_processor,cat_selector)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", RobustScaler())]\n",
    ")\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        # (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        # (\"selector\", SelectPercentile(chi2, percentile=50)),\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_col),\n",
    "        (\"cat\", categorical_transformer, cat_col),\n",
    "    ]\n",
    ")\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor)]\n",
    "    )\n",
    "\n",
    "clf.fit_transform(input_train_df,target_col)\n",
    "clf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class CustomImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, lower_thresh, upper_thresh):\n",
    "        super().__init__()\n",
    "        self.lower_thresh = lower_thresh\n",
    "        self.upper_thresh = upper_thresh\n",
    " \n",
    "    def fit(self, X, y):\n",
    "        null_prop_ = X.isnull().sum()/len(X)\n",
    "        drop_cols= null_prop_[null_prop_ > self.upper_thresh].index \n",
    "        X.drop(columns=drop_cols,inplace= True)\n",
    "\n",
    "        drop_rows = X.loc[X.isnull().mean(axis=1).between(self.lower_thresh,self.upper_thresh)].index\n",
    "        X.drop(index=drop_rows, inplace=True)\n",
    "        self.y =y\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # fill Age\n",
    "        X.fillna(X.median(),inplace=True)\n",
    "        \n",
    "        return X,self.y\n",
    "\n",
    "# pipe_obj = Pipeline([\n",
    "#     (\"CustomImputer\",imputer),\n",
    "#     (\"sampling_pipeline\", sampling_pipeline),\n",
    "#     # ('target_extractor', FunctionTransformer(lambda df: ( df[target_col_name], df.drop(columns=target_col_name)))),\n",
    "#     (\"ta\",preprocessor)\n",
    "# ])\n",
    "\n",
    "class PipelineWithY(Pipeline):\n",
    "    def transform(self, X, y=None):\n",
    "        Xt = X\n",
    "        for name, transform in self.steps[:-1]:\n",
    "            # all transformations except the last one\n",
    "            if hasattr(transform, 'transform'):\n",
    "                Xt = transform.transform(Xt)\n",
    "        # last transformation, handle differently to get X and y\n",
    "        name, last_transform = self.steps[-1]\n",
    "        if hasattr(last_transform, 'transform'):\n",
    "            Xt = last_transform.transform(Xt)\n",
    "        # return as tuple of (X, y)\n",
    "        return Xt, y\n",
    "    \n",
    "pipe_obj = PipelineWithY([\n",
    "    (\"CustomImputer\",imputer),\n",
    "    (\"sampling_pipeline\", sampling_pipeline),\n",
    "    ('target_extractor', FunctionTransformer(lambda df: ( df[target_col_name], df.drop(columns=target_col_name)))),\n",
    "    (\"ta\",preprocessor)\n",
    "])\n",
    "pipe_obj.fit(input_train_df,target_col)\n",
    "pipe_obj.transform(input_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['step'], dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc_data = pd.DataFrame(enc.fit_transform(\n",
    "    input_train_df[['type']]).toarray())\n",
    "  \n",
    "# Merge with main\n",
    "New_df = input_train_df.join(enc_data)\n",
    "New_df.drop(columns=['type'],inplace=True)\n",
    "names = enc.get_feature_names_out()\n",
    "New_df.select_dtypes(include='integer').columns\n",
    "\n",
    "# OH_X_train= pd.concat([New_df,enc_data], axis=1)\n",
    "# OH_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted, check_X_y\n",
    "class GroupImputer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class used for imputing missing values in a pd.DataFrame using either mean or median of a group.\n",
    "    \n",
    "    Parameters\n",
    "    ----------    \n",
    "    group_cols : list\n",
    "        List of columns used for calculating the aggregated value \n",
    "    target : str\n",
    "        The name of the column to impute\n",
    "    metric : str\n",
    "        The metric to be used for remplacement, can be one of ['mean', 'median']\n",
    "    Returns\n",
    "    -------\n",
    "    X : array-like\n",
    "        The array with imputed values in the target column\n",
    "    '''\n",
    "    def __init__(self, group_cols, target, metric='mean'):\n",
    "        \n",
    "        assert metric in ['mean', 'median'], 'Unrecognized value for metric, should be mean/median'\n",
    "        assert type(group_cols) == list, 'group_cols should be a list of columns'\n",
    "        assert type(target) == str, 'target should be a string'\n",
    "        \n",
    "        self.group_cols = group_cols\n",
    "        self.target = target\n",
    "        self.metric = metric\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        assert pd.isnull(X[self.group_cols]).any(axis=None) == False, 'There are missing values in group_cols'\n",
    "        \n",
    "        impute_map = X.groupby(self.group_cols)[self.target].agg(self.metric) \\\n",
    "                                                            .reset_index(drop=False)\n",
    "        \n",
    "        self.impute_map_ = impute_map\n",
    "        \n",
    "        return self \n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        # make sure that the imputer was fitted\n",
    "        check_is_fitted(self, 'impute_map_')\n",
    "        \n",
    "        X = X.copy()\n",
    "        \n",
    "        for index, row in self.impute_map_.iterrows():\n",
    "            ind = (X[self.group_cols] == row[self.group_cols]).all(axis=1)\n",
    "            X.loc[ind, self.target] = X.loc[ind, self.target].fillna(row[self.target])\n",
    "        \n",
    "        return X.values\n",
    "    \n",
    "GroupImputer(group_cols, target, metric='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with config_context(transform_output=\"pandas\"):\n",
    "    # the output of transform will be a Pandas DataFrame\n",
    "    X_test_scaled = scaler.transform(X_test[num_cols])\n",
    "X_test_scaled.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
